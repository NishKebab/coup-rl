{
  "episode": 0,
  "total_timesteps": 0,
  "training_config": {
    "episodes_per_agent_update": 10,
    "agent_pool_size": 8,
    "opponent_rotation_frequency": 50,
    "evaluation_frequency": 100,
    "save_frequency": 500,
    "max_episodes": 10000
  },
  "metrics": {
    "episode_metrics": {
      "total_episodes": 1,
      "avg_episode_length": 185.0,
      "episode_length_std": 0.0
    },
    "agent_performance": {
      "agent_2": {
        "avg_reward": 18.040000000000006,
        "reward_std": 0.0,
        "win_rate": 0.0,
        "total_games": 1,
        "reward_trend": 0
      },
      "agent_3": {
        "avg_reward": 44.95,
        "reward_std": 0.0,
        "win_rate": 0.0,
        "total_games": 1,
        "reward_trend": 0
      },
      "agent_0": {
        "avg_reward": 26.880000000000006,
        "reward_std": 0.0,
        "win_rate": 0.0,
        "total_games": 1,
        "reward_trend": 0
      },
      "agent_1": {
        "avg_reward": 169.31000000000012,
        "reward_std": 0.0,
        "win_rate": 1.0,
        "total_games": 1,
        "reward_trend": 0
      }
    },
    "action_statistics": {
      "agent_2": {
        "action_distribution": {
          "4": 0.16,
          "1": 0.12,
          "15": 0.08,
          "13": 0.08,
          "5": 0.12,
          "8": 0.04,
          "11": 0.16,
          "12": 0.04,
          "6": 0.08,
          "2": 0.04,
          "7": 0.04,
          "3": 0.04
        },
        "strategy_diversity": 0.9437937464955418,
        "most_used_action": 4,
        "total_actions": 25
      },
      "agent_3": {
        "action_distribution": {
          "15": 0.2564102564102564,
          "5": 0.05128205128205128,
          "0": 0.07692307692307693,
          "3": 0.05128205128205128,
          "4": 0.05128205128205128,
          "1": 0.1794871794871795,
          "14": 0.07692307692307693,
          "6": 0.05128205128205128,
          "7": 0.05128205128205128,
          "2": 0.07692307692307693,
          "12": 0.02564102564102564,
          "11": 0.02564102564102564,
          "9": 0.02564102564102564
        },
        "strategy_diversity": 0.8938318107930081,
        "most_used_action": 15,
        "total_actions": 39
      },
      "agent_0": {
        "action_distribution": {
          "15": 0.36,
          "6": 0.16,
          "3": 0.04,
          "5": 0.16,
          "4": 0.08,
          "14": 0.12,
          "0": 0.08
        },
        "strategy_diversity": 0.8949661724024077,
        "most_used_action": 15,
        "total_actions": 25
      },
      "agent_1": {
        "action_distribution": {
          "15": 0.11458333333333333,
          "6": 0.125,
          "0": 0.07291666666666667,
          "1": 0.10416666666666667,
          "13": 0.010416666666666666,
          "3": 0.052083333333333336,
          "2": 0.0625,
          "10": 0.03125,
          "7": 0.07291666666666667,
          "9": 0.07291666666666667,
          "12": 0.0625,
          "8": 0.03125,
          "5": 0.08333333333333333,
          "11": 0.052083333333333336,
          "4": 0.052083333333333336
        },
        "strategy_diversity": 0.9586457799909884,
        "most_used_action": 6,
        "total_actions": 96
      }
    },
    "bluffing_analysis": {},
    "training_metrics": {
      "agent_1": {
        "avg_policy_loss": -0.024273481220006943,
        "avg_value_loss": 1222.395037841797,
        "policy_loss_trend": 0,
        "training_stability": 0.0
      }
    },
    "evaluation_results": {
      "baseline_performance": {
        "agent_0": {
          "random": {
            "win_rate": 0.3333333333333333,
            "consistency": 1.0
          },
          "honest": {
            "win_rate": 0.0,
            "consistency": 1.0
          },
          "aggressive": {
            "win_rate": 0.0,
            "consistency": 1.0
          },
          "defensive": {
            "win_rate": 0.0,
            "consistency": 1.0
          }
        },
        "agent_1": {
          "random": {
            "win_rate": 0.3333333333333333,
            "consistency": 1.0
          },
          "honest": {
            "win_rate": 0.0,
            "consistency": 1.0
          },
          "aggressive": {
            "win_rate": 0.0,
            "consistency": 1.0
          },
          "defensive": {
            "win_rate": 0.0,
            "consistency": 1.0
          }
        },
        "agent_2": {
          "random": {
            "win_rate": 0.16666666666666666,
            "consistency": 1.0
          },
          "honest": {
            "win_rate": 0.0,
            "consistency": 1.0
          },
          "aggressive": {
            "win_rate": 0.0,
            "consistency": 1.0
          },
          "defensive": {
            "win_rate": 0.0,
            "consistency": 1.0
          }
        },
        "agent_3": {
          "random": {
            "win_rate": 0.08333333333333333,
            "consistency": 1.0
          },
          "honest": {
            "win_rate": 0.0,
            "consistency": 1.0
          },
          "aggressive": {
            "win_rate": 0.0,
            "consistency": 1.0
          },
          "defensive": {
            "win_rate": 0.0,
            "consistency": 1.0
          }
        }
      },
      "self_play_performance": {
        "agent_0": {
          "win_rate": 0.24,
          "trend": 0
        },
        "agent_1": {
          "win_rate": 0.24,
          "trend": 0
        },
        "agent_2": {
          "win_rate": 0.2,
          "trend": 0
        },
        "agent_3": {
          "win_rate": 0.32,
          "trend": 0
        }
      },
      "relative_strength": {}
    },
    "nash_equilibrium": {
      "strategy_convergence": 1.0,
      "estimated_exploitability": 0.5,
      "convergence_trend": 0,
      "is_converged": false,
      "equilibrium_quality": 0.5
    },
    "game_analysis": {
      "game_length_distribution": {
        "180": 1
      },
      "avg_game_length": 185.0,
      "character_usage": {},
      "endgame_patterns": {}
    }
  }
}